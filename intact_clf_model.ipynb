{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intact Medical Specialty Classification Model using NLP\n",
    "\n",
    "### By: Daniyal, Hibah, Abhishek and Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import libraries and read in the data\n",
    "\n",
    "We'll add more libraries, as we move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size with duplicates:  3969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/adamyeo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/adamyeo/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/adamyeo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/adamyeo/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "df = pd.read_csv(\"new_train.csv\", index_col=0)\n",
    "print(\"Test size with duplicates: \", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Pre-process our data\n",
    "\n",
    "I think this is the most important step here, the ML model is only as good as its dataset, so we gotta make sure it's squeaky clean.\n",
    "\n",
    "All of the basic pre-processing is done by the CountVectorizer, these tasks include:\n",
    "- Tokenize (divide words individually)\n",
    "- Remove stop-words (remove \"the, and, to, or, ...\"; other special characters)\n",
    "- Lemmatize (convert similar words into its base root; eating, eats, ate => eat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label size:  3969\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       1\n",
       "2       1\n",
       "3       2\n",
       "4       0\n",
       "       ..\n",
       "3995    4\n",
       "3996    1\n",
       "3997    1\n",
       "3998    5\n",
       "3999    1\n",
       "Name: labels, Length: 3969, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create labels/target values\n",
    "y = df.labels\n",
    "print(\"Label size: \", len(y))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train size:  2381\n",
      "y_train size:  2381\n",
      "X_test size:  1588\n",
      "y_test size:  1588\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"transcription\"], y, test_size=0.4, random_state=42)\n",
    "\n",
    "# X_train: training data of features\n",
    "print(\"X_train size: \", len(X_train))\n",
    "# y_train: training data of label\n",
    "print(\"y_train size: \", len(y_train))\n",
    "\n",
    "# X_test: test data of features\n",
    "print(\"X_test size: \", len(X_test))\n",
    "# y_test: test data of label\n",
    "print(\"y_test size: \", len(y_test))\n",
    "\n",
    "# X_train\n",
    "# y_train[:50]\n",
    "# X_test\n",
    "# y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.feature_extraction.text.CountVectorizer'>\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# Custom pre-processing function\n",
    "def preprocess_data(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d|_', '', text) # removes digits and '_'\n",
    "    text = wordnet_lemmatizer.lemmatize(text)\n",
    "    return text\n",
    "\n",
    "# , preprocessor=preprocess_data\n",
    "# Initialize a CountVectorizer object\n",
    "count_vectorizer = CountVectorizer(stop_words=\"english\", preprocessor=preprocess_data, max_df=0.3, min_df=21, ngram_range=(1, 2))\n",
    "\n",
    "print(type(count_vectorizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Fit and Transform the Data\n",
    "\n",
    "Specifically, we must fit AND transform the feature training data and only transform the feature test data.\n",
    "This is a preliminary step.\n",
    "\n",
    "In fit_transform(), what happens is that we calculate the mean and variance of the training data and standardize the entire dataset (hence, transform). We only need transform() for the test data because we are using the mean and variance of the training data to standardize the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words:  4916\n",
      "['abc' 'abcd' 'abcd general' 'abcd hospital' 'abdomen'\n",
      " 'abdomen insufflated' 'abdomen pelvis' 'abdomen prepped' 'abdomen soft'\n",
      " 'abdominal' 'abdominal aorta' 'abdominal cavity' 'abdominal pain'\n",
      " 'abdominal wall' 'abds' 'ability' 'able' 'able walk' 'abnormal'\n",
      " 'abnormalities' 'abnormalities noted' 'abnormality' 'abscess' 'absence'\n",
      " 'absent' 'abuse' 'ac' 'access' 'accessory' 'accident' 'accommodate'\n",
      " 'accommodation' 'accompanied' 'accomplished' 'according'\n",
      " 'according patient' 'ace' 'achieve' 'achieved' 'acid' 'acquired' 'active'\n",
      " 'active bleeding' 'activities' 'activities daily' 'activity' 'actually'\n",
      " 'acuity' 'acute' 'acute distress' 'adaptic' 'add' 'added' 'addition'\n",
      " 'additional' 'additionally' 'address' 'addressed' 'adenocarcinoma'\n",
      " 'adenoidectomy' 'adenopathy' 'adenopathy thyromegaly' 'adequate'\n",
      " 'adequate anesthesia' 'adequate general' 'adequate sedation' 'adequately'\n",
      " 'adherent' 'adhesions' 'adjacent' 'adjusted' 'administered'\n",
      " 'administered patient' 'administration' 'admission' 'admission patient'\n",
      " 'admit' 'admits' 'admitted' 'admitted hospital' 'admitting' 'adnexa'\n",
      " 'adnexal' 'adrenal' 'adrenal glands' 'adult' 'advair' 'advance'\n",
      " 'advanced' 'advanced left' 'adverse' 'advised' 'aerobic' 'afebrile'\n",
      " 'affect' 'affect normal' 'affected' 'aforementioned' 'african'\n",
      " 'african american' 'afternoon' 'age' 'agent' 'aggressive' 'agitation'\n",
      " 'ago' 'ago patient' 'agree' 'agreed' 'agreement' 'ahead' 'aid' 'air'\n",
      " 'air movement' 'airway' 'albumin' 'albuterol' 'alcohol' 'alcohol abuse'\n",
      " 'alcohol illicit' 'alcohol tobacco' 'alcohol use' 'alcoholic' 'alert'\n",
      " 'alert oriented' 'aligned' 'alignment' 'alkaline' 'alkaline phosphatase'\n",
      " 'allergic' 'allergies' 'allergies family' 'allergies known'\n",
      " 'allergies medications' 'allergies patient' 'allergies penicillin'\n",
      " 'allergies review' 'allergies social' 'allergy' 'allis' 'allograft'\n",
      " 'allow' 'allowed' 'allowing' 'alt' 'alternating' 'alternative'\n",
      " 'alternatives' 'ambien' 'ambulate' 'ambulating' 'ambulation' 'ambulatory'\n",
      " 'american' 'american female' 'american male' 'amounts' 'amounts sterile'\n",
      " 'amoxicillin' 'analysis' 'anastomosis' 'anatomic' 'anatomical' 'anatomy'\n",
      " 'ancef' 'anemia' 'anesthesia achieved' 'anesthesia administered'\n",
      " 'anesthesia applied' 'anesthesia bleeding' 'anesthesia care'\n",
      " 'anesthesia complications' 'anesthesia department' 'anesthesia general'\n",
      " 'anesthesia induced' 'anesthesia iv' 'anesthesia local'\n",
      " 'anesthesia obtained' 'anesthesia patient' 'anesthesia spinal'\n",
      " 'anesthetic' 'anesthetic administered' 'anesthetized' 'aneurysm' 'angina'\n",
      " 'angio' 'angio seal' 'angiogram' 'angiographic' 'angiography'\n",
      " 'angiography left' 'angioplasty' 'angle' 'angled' 'anicteric' 'ankle'\n",
      " 'ankle tourniquet' 'ankles' 'annulus' 'answer' 'answered' 'anterior'\n",
      " 'anterior abdominal' 'anterior aspect' 'anterior cervical'\n",
      " 'anterior chamber' 'anterior descending' 'anterior posterior'\n",
      " 'anteriorly' 'anteverted' 'anti' 'antibiotic' 'antibiotic solution'\n",
      " 'antibiotics' 'antibiotics given' 'anticoagulation' 'antrum' 'anus'\n",
      " 'anxiety' 'anxiety depression' 'anxious' 'aorta' 'aortic' 'aortic arch'\n",
      " 'aortic stenosis' 'aortic valve' 'ap' 'ap lateral' 'apex' 'apgars'\n",
      " 'aphasia' 'apical' 'apnea' 'apparent' 'apparent distress' 'apparently'\n",
      " 'appear' 'appearance' 'appeared' 'appeared normal' 'appearing' 'appears'\n",
      " 'appears stated' 'appendectomy' 'appendicitis' 'appendix' 'appetite'\n",
      " 'application' 'applied' 'applied patient' 'appointment' 'appreciate'\n",
      " 'appreciated' 'approach' 'appropriate' 'appropriate size' 'appropriately'\n",
      " 'approximate' 'approximated' 'approximated using' 'approximated vicryl'\n",
      " 'approximately' 'approximately cc' 'approximately cm'\n",
      " 'approximately days' 'approximately hours' 'approximately minutes'\n",
      " 'approximately ml' 'approximately mm' 'approximately months'\n",
      " 'approximately weeks' 'approximately years' 'april' 'arch' 'area'\n",
      " 'area patient' 'area prepped' 'area right' 'areas' 'arise' 'arm' 'arms'\n",
      " 'arrival' 'arrived' 'arterial' 'arteries' 'artery' 'artery bypass'\n",
      " 'artery disease' 'artery left' 'artery right' 'arthritis' 'arthroplasty'\n",
      " 'articular' 'asa' 'ascending' 'ascending aorta' 'ask' 'asked' 'asleep'\n",
      " 'aspect' 'aspect left' 'aspect right' 'aspects' 'aspirated' 'aspiration'\n",
      " 'aspirin' 'aspirin mg' 'assess' 'assessed' 'assessment'\n",
      " 'assessment patient' 'assessment plan' 'assist' 'assistance' 'assistant'\n",
      " 'assisted' 'associated' 'ast' 'ast alt' 'asthma' 'asymmetry' 'ataxia'\n",
      " 'atelectasis' 'atenolol' 'atherosclerotic' 'ativan' 'atraumatic'\n",
      " 'atraumatic normocephalic' 'atrial' 'atrial fibrillation' 'atrium'\n",
      " 'atrophy' 'attached' 'attachment' 'attachments' 'attack' 'attempt'\n",
      " 'attempted' 'attempts' 'attending' 'attention' 'attention directed'\n",
      " 'attention turned' 'atypical' 'auditory' 'augmentin' 'august'\n",
      " 'auscultation' 'auscultation bilaterally' 'auscultation cardiovascular'\n",
      " 'auscultation heart' 'auscultation percussion' 'av' 'available' 'average'\n",
      " 'avoid' 'awake' 'awake alert' 'awakened' 'awakened extubated'\n",
      " 'awakened taken' 'aware' 'away' 'axial' 'axilla' 'axillary' 'axis'\n",
      " 'axis ii' 'axis iii' 'axis iv' 'babinski' 'baby' 'bacitracin'\n",
      " 'background' 'bacteria' 'bacterial' 'bactrim' 'bag' 'balance' 'ball'\n",
      " 'balloon' 'band' 'bandage' 'bands' 'basal' 'base' 'based' 'baseline'\n",
      " 'bases' 'basically' 'basilar' 'basis' 'beats' 'beats minute' 'bed'\n",
      " 'bedside' 'bedtime' 'began' 'begin' 'beginning' 'begun' 'behavior'\n",
      " 'believe' 'believes' 'beneath' 'benefit' 'benefits'\n",
      " 'benefits alternatives' 'benefits procedure' 'benefits surgery' 'benign'\n",
      " 'best' 'beta' 'beta blocker' 'betadine' 'betadine solution' 'better'\n",
      " 'bicarbonate' 'biceps' 'biceps triceps' 'bifurcation' 'bilateral'\n",
      " 'bilateral lower' 'bilaterally' 'bilaterally abdomen' 'bile' 'bilirubin'\n",
      " 'bimanual' 'bimanual exam' 'biopsied' 'biopsies' 'biopsies taken'\n",
      " 'biopsy' 'bipolar' 'bipolar cautery' 'bipolar electrocautery' 'birth'\n",
      " 'bit' 'black' 'bladder' 'bladder flap' 'bladder function' 'bladder neck'\n",
      " 'blade' 'blade knife' 'blade scalpel' 'blade used' 'blake' 'bleed'\n",
      " 'bleeders' 'bleeding' 'bleeding infection' 'bleeding noted'\n",
      " 'bleeding seen' 'block' 'blocker' 'blood cell' 'blood cells' 'blood loss'\n",
      " 'blood pressure' 'blood sugar' 'blood sugars' 'bloody' 'blue' 'blunt'\n",
      " 'blunt dissection' 'blunt sharp' 'bluntly' 'blurred' 'blurred vision'\n",
      " 'bmp' 'bodies' 'body' 'bolus' 'bone' 'bone marrow' 'bone removed' 'bones'\n",
      " 'bony' 'bony prominences' 'border' 'borderline' 'born' 'bovie'\n",
      " 'bovie cautery' 'bovie electrocautery' 'bowel' 'bowel bladder'\n",
      " 'bowel movement' 'bowel movements' 'bowel obstruction' 'bowel sounds'\n",
      " 'bowels' 'boy' 'bp' 'bp hr' 'brachial' 'brachioradialis' 'brain']\n"
     ]
    }
   ],
   "source": [
    "# Fit and transform the TRAINING data using only the 'transciption' column values\n",
    "count_train = count_vectorizer.fit_transform(X_train.values)\n",
    "# Transform the TEST data using only the 'transciption' column values\n",
    "count_test = count_vectorizer.transform(X_test.values)\n",
    "\n",
    "\n",
    "# Print number of words processing\n",
    "print(\"Number of words: \", len(count_vectorizer.get_feature_names_out())) # number of test data from split\n",
    "# Print the features (individual tokens) of the count_vectorizer\n",
    "print(count_vectorizer.get_feature_names_out()[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Train our models here\n",
    "\n",
    "We used the Multinomial Naive Bayes to classify our labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of predictions:  1588\n",
      "[ 7  1 34 ...  1  1  1]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a Multinomial Naive Bayes classifier\n",
    "nb_clf = MultinomialNB(alpha=0.4)\n",
    "# Fit the classifier to the training data\n",
    "nb_clf.fit(count_train, y_train)\n",
    "# Create the predicted tags\n",
    "pred = nb_clf.predict(count_test)\n",
    "\n",
    "# Print the predictions for each row of the dataset (1001 rows)\n",
    "print(\"Number of predictions: \", len(pred)) # Equal to the number of test data (when it got split)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Evaluate the model\n",
    "\n",
    "We will create an accuracy score and also a confusion matrix.\n",
    "\n",
    "Precision = TP/(TP + FP)\n",
    "\n",
    "Recall = TP/(TP+FN)\n",
    "\n",
    "F1 Score = 2*(Recall * Precision) / (Recall + Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3494962216624685\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.15      0.19        33\n",
      "           1       0.34      0.18      0.23       310\n",
      "           2       0.31      0.34      0.32        77\n",
      "           3       0.34      0.59      0.43        17\n",
      "           4       0.34      0.42      0.38        60\n",
      "           5       0.38      0.46      0.41        74\n",
      "           6       0.46      0.34      0.39       115\n",
      "           7       0.52      0.37      0.43       128\n",
      "           8       0.23      0.10      0.14        29\n",
      "           9       0.47      0.57      0.52        42\n",
      "          10       0.21      0.19      0.20        97\n",
      "          11       0.13      0.11      0.12        28\n",
      "          12       0.14      0.29      0.19         7\n",
      "          13       0.17      0.25      0.21        44\n",
      "          14       0.33      0.40      0.36         5\n",
      "          15       0.53      0.74      0.62        23\n",
      "          16       0.30      0.35      0.32       169\n",
      "          17       0.00      0.00      0.00         2\n",
      "          18       0.31      0.62      0.41        26\n",
      "          19       0.41      0.63      0.50        51\n",
      "          20       0.40      0.40      0.40        58\n",
      "          21       0.41      0.62      0.49        34\n",
      "          22       0.40      1.00      0.57         2\n",
      "          23       0.50      0.22      0.31         9\n",
      "          24       0.33      0.29      0.31         7\n",
      "          25       0.22      0.50      0.30        14\n",
      "          26       0.00      0.00      0.00         2\n",
      "          27       0.49      0.75      0.59        28\n",
      "          28       0.17      1.00      0.29         1\n",
      "          29       0.45      0.83      0.59         6\n",
      "          30       0.22      0.32      0.26        25\n",
      "          31       0.29      0.25      0.27         8\n",
      "          32       0.25      0.17      0.20         6\n",
      "          33       0.33      0.25      0.29         8\n",
      "          34       0.64      0.79      0.71        29\n",
      "          35       0.38      0.60      0.46         5\n",
      "          36       0.00      0.00      0.00         2\n",
      "          37       0.50      0.60      0.55         5\n",
      "          38       0.07      1.00      0.13         1\n",
      "          39       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.35      1588\n",
      "   macro avg       0.31      0.42      0.33      1588\n",
      "weighted avg       0.36      0.35      0.34      1588\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "# Calculate the confusion matrix\n",
    "# conf_matrix = metrics.confusion_matrix(y_test, pred)\n",
    "\n",
    "print(score)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
