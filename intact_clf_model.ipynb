{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intact Medical Specialty Classification Model using NLP\n",
    "\n",
    "### By: Daniyal, Hibah, Abhishek and Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import libraries and read in the data\n",
    "\n",
    "We'll add more libraries, as we move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/adamyeo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/adamyeo/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size:  3969\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>medical_specialty</th>\n",
       "      <th>transcription</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Emergency Room Reports</td>\n",
       "      <td>REASON FOR THE VISIT:,  Very high PT/INR.,HIST...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Surgery</td>\n",
       "      <td>PREOPERATIVE DIAGNOSIS:,  Acetabular fracture ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Surgery</td>\n",
       "      <td>NAME OF PROCEDURE,1.  Selective coronary angio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Radiology</td>\n",
       "      <td>REFERRING DIAGNOSIS: , Motor neuron disease.,P...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Emergency Room Reports</td>\n",
       "      <td>CHIEF COMPLAINT: , Dental pain.,HISTORY OF PRE...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>Neurology</td>\n",
       "      <td>PROBLEMS AND ISSUES:,1.  Headaches, nausea, an...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>Surgery</td>\n",
       "      <td>PREOPERATIVE DIAGNOSIS: , Anemia.,PROCEDURE:, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>Surgery</td>\n",
       "      <td>1.  Odynophagia.,2.  Dysphagia.,3.  Gastroesop...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>Gastroenterology</td>\n",
       "      <td>The patient's abdomen was prepped and draped i...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>Surgery</td>\n",
       "      <td>PREOPERATIVE DIAGNOSIS: , Right pleural effusi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3969 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            medical_specialty  \\\n",
       "0      Emergency Room Reports   \n",
       "1                     Surgery   \n",
       "2                     Surgery   \n",
       "3                   Radiology   \n",
       "4      Emergency Room Reports   \n",
       "...                       ...   \n",
       "3995                Neurology   \n",
       "3996                  Surgery   \n",
       "3997                  Surgery   \n",
       "3998         Gastroenterology   \n",
       "3999                  Surgery   \n",
       "\n",
       "                                          transcription  labels  \n",
       "0     REASON FOR THE VISIT:,  Very high PT/INR.,HIST...       0  \n",
       "1     PREOPERATIVE DIAGNOSIS:,  Acetabular fracture ...       1  \n",
       "2     NAME OF PROCEDURE,1.  Selective coronary angio...       1  \n",
       "3     REFERRING DIAGNOSIS: , Motor neuron disease.,P...       2  \n",
       "4     CHIEF COMPLAINT: , Dental pain.,HISTORY OF PRE...       0  \n",
       "...                                                 ...     ...  \n",
       "3995  PROBLEMS AND ISSUES:,1.  Headaches, nausea, an...       4  \n",
       "3996  PREOPERATIVE DIAGNOSIS: , Anemia.,PROCEDURE:, ...       1  \n",
       "3997  1.  Odynophagia.,2.  Dysphagia.,3.  Gastroesop...       1  \n",
       "3998  The patient's abdomen was prepped and draped i...       5  \n",
       "3999  PREOPERATIVE DIAGNOSIS: , Right pleural effusi...       1  \n",
       "\n",
       "[3969 rows x 3 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import spacy\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "df = pd.read_csv(\"new_train.csv\", index_col=0)\n",
    "print(\"Test size: \", len(df))\n",
    "df # print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Pre-process our data\n",
    "\n",
    "I think this is the most important step here, the ML model is only as good as its dataset, so we gotta make sure it's squeaky clean.\n",
    "\n",
    "All of the basic pre-processing is done by the CountVectorizer, these tasks include:\n",
    "- Tokenize (divide words individually)\n",
    "- Remove stop-words (remove \"the, and, to, or, ...\"; other special characters)\n",
    "- Lemmatize (convert similar words into its base root; eating, eats, ate => eat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label size:  3969\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       1\n",
       "2       1\n",
       "3       2\n",
       "4       0\n",
       "       ..\n",
       "3995    4\n",
       "3996    1\n",
       "3997    1\n",
       "3998    5\n",
       "3999    1\n",
       "Name: labels, Length: 3969, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create labels/target values\n",
    "y = df.labels\n",
    "print(\"Label size: \", len(y))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train size:  3572\n",
      "y_train size:  3572\n",
      "X_test size:  397\n",
      "y_test size:  397\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"transcription\"], y, test_size=0.1, random_state=53)\n",
    "\n",
    "# X_train: training data of features\n",
    "print(\"X_train size: \", len(X_train))\n",
    "# y_train: training data of label\n",
    "print(\"y_train size: \", len(y_train))\n",
    "\n",
    "# X_test: test data of features\n",
    "print(\"X_test size: \", len(X_test))\n",
    "# y_test: test data of label\n",
    "print(\"y_test size: \", len(y_test))\n",
    "\n",
    "# X_train\n",
    "# y_train[:50]\n",
    "# X_test\n",
    "# y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strawberries\n",
      "strawberry\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "# we could try stemming as well\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "all_stopwords = nlp.Defaults.stop_words\n",
    "My_text = \"strawberries\"\n",
    "doc = nlp(My_text)\n",
    "print(doc)\n",
    "\n",
    "# Custom pre-processing function\n",
    "def preprocess_data(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+|_', '', text) # removes words with digits and '_'\n",
    "    wordnet_lemmatizer.lemmatize(text)\n",
    "    return text\n",
    "\n",
    "# Initialize a CountVectorizer object\n",
    "count_vectorizer = CountVectorizer(stop_words=\"english\", preprocessor=preprocess_data) # work on more pre-processing\n",
    "\n",
    "print(wordnet_lemmatizer.lemmatize(\"strawberries\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Fit and Transform the Data\n",
    "\n",
    "Specifically, we must fit AND transform the feature training data and only transform the feature test data.\n",
    "This is a preliminary step.\n",
    "\n",
    "In fit_transform(), what happens is that we calculate the mean and variance of the training data and standardize the entire dataset (hence, transform). We only need transform() for the test data because we are using the mean and variance of the training data to standardize the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20049\n",
      "['aa' 'ab' 'abadeedleedlebadle' 'abandoned' 'abandonment' 'abated'\n",
      " 'abbott' 'abbreviated' 'abc' 'abcd' 'abcg' 'abciximab' 'abd' 'abdomen'\n",
      " 'abdominal' 'abdominally' 'abdominis' 'abdominopelvic' 'abdominoplasty'\n",
      " 'abdominosacrocolpopexy' 'abdominus' 'abds' 'abduct' 'abducted'\n",
      " 'abduction' 'abducto' 'abductor' 'abductors' 'abductovalgus' 'abductus'\n",
      " 'aberrant' 'aberration' 'abf' 'abg' 'abgs' 'abilify' 'abilities'\n",
      " 'ability' 'ablate' 'ablated' 'ablation' 'ablative' 'able' 'abnormal'\n",
      " 'abnormalities' 'abnormality' 'abnormally' 'abnormities' 'abo' 'abolish'\n",
      " 'abort' 'aborted' 'abortion' 'abortions' 'abortive' 'abovementioned'\n",
      " 'abraded' 'abrading' 'abraham' 'abrasion' 'abrasions' 'abraxane'\n",
      " 'abreast' 'abrogated' 'abrogation' 'abrupt' 'abruptio' 'abruptly' 'abs'\n",
      " 'abscess' 'abscessed' 'abscesses' 'absence' 'absent' 'absolute'\n",
      " 'absolutely' 'absorb' 'absorbable' 'absorbables' 'absorbing' 'absorption'\n",
      " 'absorptive' 'abstain' 'abstinence' 'abstract' 'abstraction'\n",
      " 'abstractions' 'abundant' 'abundantly' 'abuse' 'abused' 'abuser'\n",
      " 'abusing' 'abusive' 'abut' 'abutments' 'abuts' 'abutting' 'ac' 'aca'\n",
      " 'academic' 'academy' 'acalculous' 'acanthosis' 'acathexia' 'accelerate'\n",
      " 'accelerated' 'accelerating' 'acceleration' 'accelerations' 'accentuated'\n",
      " 'accept' 'acceptable' 'accepted' 'accepting' 'accepts' 'access'\n",
      " 'accessed' 'accesses' 'accessing' 'accessory' 'accident' 'accidental'\n",
      " 'accidentally' 'accidents' 'accommodate' 'accommodated' 'accommodation'\n",
      " 'accommodative' 'accompanied' 'accompanies' 'accompany' 'accompanying'\n",
      " 'accomplish' 'accomplished' 'accomplishing' 'according' 'accordingly'\n",
      " 'account' 'accounted' 'accounting' 'accounts' 'accu' 'accumulate'\n",
      " 'accumulating' 'accumulation' 'accumulations' 'accupril' 'accuracy'\n",
      " 'accurate' 'accurately' 'accurus' 'accusation' 'accutane' 'acdf' 'ace'\n",
      " 'acellular' 'acetabular' 'acetabulum' 'acetaminophen' 'acetate'\n",
      " 'acetazolamide' 'acetic' 'aceto' 'acetowhite' 'acetyl' 'acetylcholine'\n",
      " 'acf' 'achalasia' 'ache' 'aches' 'achieve' 'achieved' 'achievement'\n",
      " 'achieves' 'achieving' 'achilles' 'achiness' 'aching' 'achy' 'acid'\n",
      " 'acidic' 'acidophilus' 'acidosis' 'acidotic' 'acids' 'aciphex' 'ackerman'\n",
      " 'acknowledge' 'acknowledged' 'acknowledging' 'acl' 'acls' 'acmi' 'acne'\n",
      " 'acneiform' 'acoustic' 'acquired' 'acquisition' 'acromegaly' 'acromial'\n",
      " 'acromioclavicular' 'acromion' 'acromionizer' 'acromioplasty' 'acrylic'\n",
      " 'acrysof' 'acs' 'act' 'acted' 'acticoat' 'acting' 'actinic' 'action'\n",
      " 'actions' 'activase' 'activated' 'activation' 'activator' 'active'\n",
      " 'actively' 'activities' 'activity' 'actonel' 'actoplus' 'actos' 'acts'\n",
      " 'actual' 'actually' 'acufix' 'acuities' 'acuity' 'acular' 'acumed'\n",
      " 'acuminata' 'acupuncture' 'acusticus' 'acute' 'acutely' 'acyanotic'\n",
      " 'acyclovir' 'ad' 'ada' 'adamant' 'adamantly' 'adapted' 'adapter'\n",
      " 'adaptic' 'adaptor' 'add' 'added' 'addendum' 'adderall' 'addict'\n",
      " 'addicted' 'addiction' 'addictive' 'adding' 'addison' 'addition'\n",
      " 'additional' 'additionally' 'additive' 'addr' 'address' 'addressed'\n",
      " 'addresses' 'addressing' 'adds' 'adducted' 'adduction' 'adductor'\n",
      " 'adductors' 'adema' 'adeno' 'adenocarcinoma' 'adenoid' 'adenoidal'\n",
      " 'adenoidectomy' 'adenoiditis' 'adenoids' 'adenoma' 'adenomas'\n",
      " 'adenomatous' 'adenomyomatosis' 'adenomyosis' 'adenopathy' 'adenosarcoma'\n",
      " 'adenosine' 'adenotonsillar' 'adenotonsillectomy' 'adenotonsillitis'\n",
      " 'adequacy' 'adequate' 'adequately' 'adhd' 'adhere' 'adhered' 'adherence'\n",
      " 'adherent' 'adherently' 'adhering' 'adhesed' 'adhesiolysis' 'adhesion'\n",
      " 'adhesions' 'adhesive' 'adhesives' 'adipose' 'adiposity' 'adjacent'\n",
      " 'adjoining' 'adjudicate' 'adjunct' 'adjunctive' 'adjust' 'adjustable'\n",
      " 'adjusted' 'adjusting' 'adjustment' 'adjustments' 'adjusts' 'adjuvant'\n",
      " 'adl' 'adls' 'administer' 'administered' 'administering' 'administrated'\n",
      " 'administration' 'administrative' 'administrator' 'admission'\n",
      " 'admissions' 'admit' 'admits' 'admitted' 'admittedly' 'admitting'\n",
      " 'admixed' 'admixture' 'adnexa' 'adnexae' 'adnexal' 'adolescence'\n",
      " 'adolescent' 'adopted' 'adoption' 'adrenal' 'adrenalectomy' 'adrenalin'\n",
      " 'adrenaline' 'adrenals' 'adrenocot' 'adriamycin' 'adson' 'adult'\n",
      " 'adulthood' 'adults' 'advair' 'advance' 'advanced' 'advancement'\n",
      " 'advancements' 'advancing' 'advantage' 'advantages' 'advent'\n",
      " 'adventitial' 'adventitious' 'adverse' 'advice' 'advil' 'advise'\n",
      " 'advised' 'advises' 'ae' 'aerated' 'aeration' 'aerobic' 'aerobics'\n",
      " 'aerosol' 'aerosols' 'aesculap' 'aesthetic' 'af' 'afb' 'afebrile'\n",
      " 'affair' 'affect' 'affected' 'affecting' 'affection' 'affective'\n",
      " 'affectively' 'affects' 'afferent' 'affidavit' 'affix' 'affixed'\n",
      " 'affixing' 'afford' 'affordable' 'afforded' 'afi' 'afib' 'afo' 'afof'\n",
      " 'afore' 'aforementioned' 'afos' 'afraid' 'african' 'afrin' 'aftercare'\n",
      " 'afternoon' 'aftershave' 'afterward' 'ag' 'agammaglobulinemia' 'agarose'\n",
      " 'agatston' 'agcl' 'age' 'aged' 'agee' 'ageing' 'agencies' 'agency'\n",
      " 'agent' 'agents' 'ages' 'agglutinins' 'aggravate' 'aggravated'\n",
      " 'aggravates' 'aggravating' 'aggravation' 'aggregate' 'aggregates'\n",
      " 'aggregation' 'aggregations' 'aggrenox' 'aggressive' 'aggressively'\n",
      " 'aging' 'agitated' 'agitation' 'agnosia' 'ago' 'agoraphobia' 'agree'\n",
      " 'agreeable' 'agreed' 'agreement' 'agrees' 'ah' 'ahead' 'ahi' 'ahmed' 'ai'\n",
      " 'aic' 'aica' 'aicd' 'aid' 'aide' 'aided' 'aiding' 'aidp' 'aids' 'aimed'\n",
      " 'air' 'airbag' 'airdrop' 'aired' 'airflow' 'airing' 'airlifted' 'airline'\n",
      " 'airplane' 'airplanes' 'airspace' 'airtight' 'airway' 'airways' 'aka'\n",
      " 'akin' 'akinesia' 'akinesis' 'akinetic' 'al' 'ala' 'alae' 'alameda'\n",
      " 'alar' 'alba' 'albeit' 'albicantia' 'albuginea' 'albumin']\n"
     ]
    }
   ],
   "source": [
    "# Fit and transform the TRAINING data using only the 'transciption' column values\n",
    "count_train = count_vectorizer.fit_transform(X_train.values)\n",
    "# Transform the TEST data using only the 'transciption' column values\n",
    "count_test = count_vectorizer.transform(X_test.values)\n",
    "\n",
    "\n",
    "# Print number of words processing\n",
    "print(len(count_vectorizer.get_feature_names_out()))\n",
    "# Print the features (individual tokens) of the count_vectorizer\n",
    "print(count_vectorizer.get_feature_names_out()[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Train our models here\n",
    "\n",
    "We used the Multinomial Naive Bayes to classify our labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30 16  9  6 20 18 15 13  6 10 18  7  6  7 20 16  1  1 21  1 15  6 16 27\n",
      "  5 16 16  7  1  5 21 16  4 16 10  5 18  6  4  7  1 16  5  1  7  1  1  5\n",
      "  6 13  1  1  1  2  1  1  9 20  7  7 18  6 16  6  2 10 21  1 10 27  1  9\n",
      "  6 13  2 10  4  5 18 16  7  1 16  7 19  8  6  1  4  6 16  4 16 16  9 16\n",
      "  2 16 16  7]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a Multinomial Naive Bayes classifier\n",
    "nb_clf = MultinomialNB()\n",
    "# Fit the classifier to the training data\n",
    "nb_clf.fit(count_train, y_train)\n",
    "# Create the predicted tags\n",
    "pred = nb_clf.predict(count_test)\n",
    "\n",
    "# Print the predictions for each row of the dataset (1001 rows)\n",
    "print(pred[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Evaluate the model\n",
    "\n",
    "We will create an accuracy score and also a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2972292191435768\n",
      "[[ 0  0  0 ...  0  0  0]\n",
      " [ 0 27  0 ...  1  0  0]\n",
      " [ 0  0  5 ...  1  0  0]\n",
      " ...\n",
      " [ 0  1  0 ...  1  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         4\n",
      "           1       0.42      0.28      0.33        97\n",
      "           2       0.28      0.20      0.23        25\n",
      "           3       0.00      0.00      0.00         3\n",
      "           4       0.33      0.32      0.32        19\n",
      "           5       0.31      0.24      0.27        17\n",
      "           6       0.22      0.29      0.25        24\n",
      "           7       0.35      0.47      0.40        30\n",
      "           8       0.00      0.00      0.00         5\n",
      "           9       0.40      0.67      0.50         6\n",
      "          10       0.13      0.24      0.17        17\n",
      "          11       0.00      0.00      0.00         7\n",
      "          12       0.00      0.00      0.00         1\n",
      "          13       0.00      0.00      0.00        12\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.57      1.00      0.73         4\n",
      "          16       0.29      0.50      0.37        50\n",
      "          17       0.00      0.00      0.00         1\n",
      "          18       0.14      0.50      0.22         4\n",
      "          19       0.30      0.38      0.33         8\n",
      "          20       0.20      0.14      0.17        14\n",
      "          21       0.42      0.50      0.45        10\n",
      "          22       1.00      1.00      1.00         1\n",
      "          23       0.00      0.00      0.00         4\n",
      "          24       0.00      0.00      0.00         1\n",
      "          25       0.00      0.00      0.00         5\n",
      "          27       0.50      0.50      0.50         6\n",
      "          28       0.00      0.00      0.00         1\n",
      "          29       0.00      0.00      0.00         2\n",
      "          30       0.00      0.00      0.00         6\n",
      "          31       0.00      0.00      0.00         3\n",
      "          32       0.00      0.00      0.00         2\n",
      "          33       0.00      0.00      0.00         2\n",
      "          34       0.33      0.50      0.40         2\n",
      "          35       0.00      0.00      0.00         1\n",
      "          37       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.30       397\n",
      "   macro avg       0.20      0.24      0.21       397\n",
      "weighted avg       0.28      0.30      0.28       397\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adamyeo/opt/miniconda3/envs/BASICBIO/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/adamyeo/opt/miniconda3/envs/BASICBIO/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/adamyeo/opt/miniconda3/envs/BASICBIO/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = metrics.confusion_matrix(y_test, pred)\n",
    "\n",
    "print(score)\n",
    "print(conf_matrix)\n",
    "print(classification_report(y_test, pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "9ce98d6a59ba4c585b822c83f97ca15f1c905b852a0710f5a850035c25558f04"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
