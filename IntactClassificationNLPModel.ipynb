{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intact Medical Data ML model using Naive Bayes Classification Model\n",
    "\n",
    "### By: Daniyal, Hibah, Abhishek and Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our Data Hackathon project, we were given medical transcription data by Intact to predict which of the 30 provided medical specialties it should be sent to. We are judged based off the macro f-score. Here are the steps we took to maximize the f-score:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries and read in the Dataset to train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/daniyalmohammed/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/daniyalmohammed/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/daniyalmohammed/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/daniyalmohammed/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'new_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maveraged_perceptron_tagger\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m## Reading in our data\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnew_train.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest size with duplicates: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(df))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1736\u001b[0m     f,\n\u001b[1;32m   1737\u001b[0m     mode,\n\u001b[1;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1744\u001b[0m )\n\u001b[1;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'new_train.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "## Reading in our data\n",
    "df = pd.read_csv(\"new_train.csv\", index_col=0)\n",
    "print(\"Test size with duplicates: \", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Pre-process our data\n",
    "\n",
    "I think this is the most important step here, the ML model is only as good as its dataset, so we gotta make sure it's squeaky clean.\n",
    "\n",
    "All of the basic pre-processing is done by the CountVectorizer, these tasks include:\n",
    "- Tokenize (divide words individually)\n",
    "- Remove stop-words (remove \"the, and, to, or, ...\"; other special characters)\n",
    "- Lemmatize (convert similar words into its base root; eating, eats, ate => eat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label size:  3969\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       1\n",
       "2       1\n",
       "3       2\n",
       "4       0\n",
       "       ..\n",
       "3995    4\n",
       "3996    1\n",
       "3997    1\n",
       "3998    5\n",
       "3999    1\n",
       "Name: labels, Length: 3969, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create labels/target values\n",
    "y = df.labels\n",
    "print(\"Label size: \", len(y))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train size:  1587\n",
      "y_train size:  1587\n",
      "X_test size:  2382\n",
      "y_test size:  2382\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"transcription\"], y, test_size=0.6, random_state=42)\n",
    "\n",
    "# X_train: training data of features\n",
    "print(\"X_train size: \", len(X_train))\n",
    "# y_train: training data of label\n",
    "print(\"y_train size: \", len(y_train))\n",
    "\n",
    "# X_test: test data of features\n",
    "print(\"X_test size: \", len(X_test))\n",
    "# y_test: test data of label\n",
    "print(\"y_test size: \", len(y_test))\n",
    "\n",
    "# X_train\n",
    "# y_train[:50]\n",
    "# X_test\n",
    "# y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.feature_extraction.text.CountVectorizer'>\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# Custom pre-processing function\n",
    "def preprocess_data(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d|_', '', text) # removes digits and '_'\n",
    "    text = wordnet_lemmatizer.lemmatize(text)\n",
    "    return text\n",
    "\n",
    "# , preprocessor=preprocess_data\n",
    "# Initialize a CountVectorizer object\n",
    "count_vectorizer = CountVectorizer(stop_words=\"english\", preprocessor=preprocess_data, max_df=0.2, min_df=25, ngram_range=(1, 2))\n",
    "\n",
    "print(type(count_vectorizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Fit and Transform the Data\n",
    "\n",
    "Specifically, we must fit AND transform the feature training data and only transform the feature test data.\n",
    "This is a preliminary step.\n",
    "\n",
    "In fit_transform(), what happens is that we calculate the mean and variance of the training data and standardize the entire dataset (hence, transform). We only need transform() for the test data because we are using the mean and variance of the training data to standardize the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adamyeo/opt/miniconda3/envs/BASICBIO/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words:  3315\n",
      "['abc' 'abcd' 'abcd general' 'abdomen' 'abdomen pelvis' 'abdomen prepped'\n",
      " 'abdomen soft' 'abdominal' 'abdominal cavity' 'abdominal pain'\n",
      " 'abdominal wall' 'ability' 'able' 'abnormal' 'abnormalities'\n",
      " 'abnormality' 'abscess' 'absent' 'abuse' 'ac' 'access' 'accident'\n",
      " 'accommodate' 'accommodation' 'accompanied' 'accomplished' 'according'\n",
      " 'ace' 'achieved' 'acid' 'active' 'activities' 'activity' 'actually'\n",
      " 'acute' 'acute distress' 'adaptic' 'add' 'added' 'addition' 'additional'\n",
      " 'additionally' 'adenocarcinoma' 'adenopathy' 'adequate'\n",
      " 'adequate general' 'adequately' 'adhesions' 'adjacent' 'administered'\n",
      " 'administered patient' 'administration' 'admission' 'admit' 'admitted'\n",
      " 'admitted hospital' 'admitting' 'adnexal' 'adrenal' 'adult' 'advanced'\n",
      " 'advised' 'afebrile' 'affect' 'african' 'african american' 'afternoon'\n",
      " 'age' 'aggressive' 'ago' 'ago patient' 'agree' 'agreed' 'ahead' 'aid'\n",
      " 'air' 'airway' 'albumin' 'albuterol' 'alcohol' 'alcohol use' 'alert'\n",
      " 'alert oriented' 'alignment' 'allergic' 'allergies' 'allergies known'\n",
      " 'allergies medications' 'allergies penicillin' 'allergies social'\n",
      " 'allergy' 'allis' 'allograft' 'allow' 'allowed' 'allowing' 'alt'\n",
      " 'alternative' 'alternatives' 'ambulate' 'ambulation' 'american'\n",
      " 'american female' 'amounts' 'analysis' 'anastomosis' 'anatomic' 'anatomy'\n",
      " 'ancef' 'anemia' 'anesthesia administered' 'anesthesia care'\n",
      " 'anesthesia general' 'anesthesia induced' 'anesthesia local'\n",
      " 'anesthesia obtained' 'anesthesia patient' 'anesthetic' 'anesthetized'\n",
      " 'angina' 'angiogram' 'angiography' 'angioplasty' 'angle' 'anicteric'\n",
      " 'ankle' 'ankles' 'answered' 'anterior' 'anterior aspect'\n",
      " 'anterior cervical' 'anterior chamber' 'anterior descending'\n",
      " 'anterior posterior' 'anteriorly' 'anti' 'antibiotic' 'antibiotics'\n",
      " 'anticoagulation' 'anxiety' 'aorta' 'aortic' 'aortic valve' 'ap'\n",
      " 'ap lateral' 'apex' 'apical' 'apnea' 'apparent' 'apparent distress'\n",
      " 'apparently' 'appear' 'appearance' 'appeared' 'appeared normal'\n",
      " 'appearing' 'appears' 'appendectomy' 'appendix' 'appetite' 'application'\n",
      " 'applied' 'applied patient' 'appointment' 'appreciated' 'approach'\n",
      " 'appropriate' 'appropriately' 'approximate' 'approximated'\n",
      " 'approximated vicryl' 'approximately' 'approximately cc'\n",
      " 'approximately cm' 'approximately days' 'approximately hours'\n",
      " 'approximately minutes' 'approximately ml' 'approximately mm'\n",
      " 'approximately weeks' 'approximately years' 'arch' 'area' 'area patient'\n",
      " 'area prepped' 'areas' 'arm' 'arms' 'arterial' 'arteries' 'artery'\n",
      " 'artery bypass' 'artery disease' 'artery left' 'arthritis' 'arthroplasty'\n",
      " 'articular' 'ascending' 'ascending aorta' 'asked' 'aspect' 'aspect right'\n",
      " 'aspects' 'aspirated' 'aspiration' 'aspirin' 'aspirin mg' 'assess'\n",
      " 'assessed' 'assessment' 'assessment patient' 'assessment plan' 'assist'\n",
      " 'assistance' 'assistant' 'assisted' 'associated' 'ast' 'ast alt' 'asthma'\n",
      " 'asymmetry' 'ataxia' 'atelectasis' 'atraumatic'\n",
      " 'atraumatic normocephalic' 'atrial' 'atrial fibrillation' 'atrium'\n",
      " 'atrophy' 'attached' 'attachment' 'attachments' 'attack' 'attempt'\n",
      " 'attempted' 'attempts' 'attention' 'attention directed'\n",
      " 'attention turned' 'atypical' 'auditory' 'august' 'auscultation'\n",
      " 'auscultation bilaterally' 'auscultation percussion' 'available' 'avoid'\n",
      " 'awake' 'awake alert' 'awakened' 'awakened taken' 'aware' 'away' 'axial'\n",
      " 'axillary' 'axis' 'babinski' 'baby' 'bacitracin' 'background' 'bag'\n",
      " 'balance' 'ball' 'balloon' 'band' 'bandage' 'base' 'based' 'baseline'\n",
      " 'bases' 'basis' 'beats' 'bed' 'bedside' 'bedtime' 'began' 'beginning'\n",
      " 'begun' 'behavior' 'believe' 'believes' 'benefit' 'benefits'\n",
      " 'benefits alternatives' 'benefits procedure' 'benign' 'best' 'beta'\n",
      " 'betadine' 'better' 'bicarbonate' 'biceps' 'bifurcation' 'bilateral'\n",
      " 'bilateral lower' 'bilaterally' 'bimanual' 'biopsies' 'biopsies taken'\n",
      " 'biopsy' 'bipolar' 'birth' 'bit' 'black' 'bladder' 'blade'\n",
      " 'blade scalpel' 'blade used' 'bleed' 'bleeding' 'bleeding infection'\n",
      " 'block' 'blood cell' 'blood cells' 'blood loss' 'blood pressure' 'blunt'\n",
      " 'blunt dissection' 'bluntly' 'bodies' 'body' 'bolus' 'bone' 'bones'\n",
      " 'bony' 'bony prominences' 'border' 'borderline' 'born' 'bovie'\n",
      " 'bovie cautery' 'bovie electrocautery' 'bowel' 'bowel bladder'\n",
      " 'bowel sounds' 'boy' 'bp' 'bp hr' 'brachial' 'brain' 'branch' 'branches'\n",
      " 'breast' 'breast cancer' 'breasts' 'breath' 'breath chest'\n",
      " 'breath sounds' 'breathing' 'brief' 'brief history' 'briefly' 'broad'\n",
      " 'bronchoscopy' 'brother' 'brought' 'brought operating'\n",
      " 'brought operative' 'brought recovery' 'bruising' 'bruit' 'bruits' 'bulb'\n",
      " 'bulging' 'bulk' 'bun' 'bun creatinine' 'bur' 'burning' 'bypass'\n",
      " 'calcification' 'calcified' 'calcium' 'calculated' 'calf' 'caliber'\n",
      " 'called' 'came' 'camera' 'canal' 'canals' 'cancer' 'candidate' 'cannula'\n",
      " 'cannulate' 'cannulated' 'capillary' 'capillary refill' 'capsular'\n",
      " 'capsule' 'carcinoma' 'cardiac' 'cardiac catheterization' 'cardiology'\n",
      " 'cardiopulmonary' 'cardiovascular' 'cardiovascular heart'\n",
      " 'cardiovascular regular' 'care' 'care patient' 'care physician'\n",
      " 'care taken' 'care unit' 'careful' 'carefully' 'carotid' 'carotid artery'\n",
      " 'carotid bruits' 'carpal' 'carpal tunnel' 'carried' 'cartilage' 'case'\n",
      " 'case patient' 'cataract' 'cath' 'catheter' 'catheter advanced'\n",
      " 'catheter inserted' 'catheter placed' 'catheter removed'\n",
      " 'catheterization' 'catheters' 'caucasian' 'caucasian female'\n",
      " 'caucasian male' 'cause' 'caused' 'causing' 'cauterized' 'cautery'\n",
      " 'cautery used' 'cava' 'cavity' 'cbc' 'cc' 'cc marcaine' 'cecum' 'cell'\n",
      " 'cell carcinoma' 'cell count' 'cells' 'center' 'central' 'cephalad'\n",
      " 'cephalic' 'cerebellar' 'cerebral' 'certainly' 'cervical'\n",
      " 'cervical discectomy' 'cervical spine' 'cervix' 'chamber' 'change'\n",
      " 'changed' 'changes' 'chart' 'check' 'checked' 'chemotherapy' 'chest'\n",
      " 'chest clear' 'chest pain' 'chest ray' 'chest wall' 'chief'\n",
      " 'chief complaint' 'child' 'children' 'chills' 'chloride'\n",
      " 'cholecystectomy' 'cholesterol' 'chromic' 'chronic' 'chronic obstructive'\n",
      " 'chronic pain' 'cigarettes' 'circumflex' 'clamp' 'clamped' 'clamps'\n",
      " 'clean' 'clean dry' 'cleaned' 'cleansed' 'clear' 'clear auscultation'\n",
      " 'clear heart' 'clear neck' 'cleared' 'clearly' 'clinic' 'clinical'\n",
      " 'clinical history' 'clinically' 'clips' 'clock' 'clock position']\n"
     ]
    }
   ],
   "source": [
    "# Fit and transform the TRAINING data using only the 'transciption' column values\n",
    "count_train = count_vectorizer.fit_transform(X_train.values)\n",
    "# Transform the TEST data using only the 'transciption' column values\n",
    "count_test = count_vectorizer.transform(X_test.values)\n",
    "\n",
    "\n",
    "# Print number of words processing\n",
    "print(\"Number of words: \", len(count_vectorizer.get_feature_names_out())) # number of test data from split\n",
    "# Print the features (individual tokens) of the count_vectorizer\n",
    "print(count_vectorizer.get_feature_names_out()[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Train our models here\n",
    "\n",
    "From our previous work, we can see that the Multinomial Naive Bayes is the most accurate model to classify our labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of predictions:  2382\n",
      "[ 7  1 34 ...  9  3  1]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a Multinomial Naive Bayes classifier\n",
    "nb_clf = MultinomialNB(alpha=0.01)\n",
    "# Fit the classifier to the training data\n",
    "nb_clf.fit(count_train, y_train)\n",
    "# Create the predicted tags\n",
    "pred = nb_clf.predict(count_test)\n",
    "\n",
    "# Print the predictions for each row of the dataset (1001 rows)\n",
    "print(\"Number of predictions: \", len(pred)) # Equal to the number of test data (when it got split)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Evaluate the model\n",
    "\n",
    "We will create an accuracy score and also a confusion matrix.\n",
    "\n",
    "Precision = TP/(TP + FP)\n",
    "\n",
    "Recall = TP/(TP+FN)\n",
    "\n",
    "F1 Score = 2*(Recall * Precision) / (Recall + Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3513853904282116\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.07      0.10        42\n",
      "           1       0.35      0.23      0.28       485\n",
      "           2       0.38      0.37      0.37       119\n",
      "           3       0.29      0.50      0.37        24\n",
      "           4       0.35      0.38      0.37        95\n",
      "           5       0.39      0.42      0.40       106\n",
      "           6       0.44      0.39      0.41       174\n",
      "           7       0.52      0.31      0.39       196\n",
      "           8       0.13      0.08      0.10        38\n",
      "           9       0.47      0.51      0.49        53\n",
      "          10       0.18      0.28      0.22       132\n",
      "          11       0.22      0.17      0.19        41\n",
      "          12       0.26      0.36      0.30        14\n",
      "          13       0.25      0.27      0.26        73\n",
      "          14       0.27      0.43      0.33         7\n",
      "          15       0.57      0.71      0.63        34\n",
      "          16       0.31      0.33      0.32       254\n",
      "          17       0.00      0.00      0.00         3\n",
      "          18       0.36      0.53      0.43        47\n",
      "          19       0.41      0.61      0.49        76\n",
      "          20       0.40      0.40      0.40        87\n",
      "          21       0.39      0.62      0.47        52\n",
      "          22       0.67      0.80      0.73         5\n",
      "          23       0.17      0.08      0.11        12\n",
      "          24       0.40      0.18      0.25        11\n",
      "          25       0.23      0.38      0.29        24\n",
      "          26       0.00      0.00      0.00         2\n",
      "          27       0.46      0.78      0.58        40\n",
      "          28       0.25      1.00      0.40         3\n",
      "          29       0.39      0.78      0.52         9\n",
      "          30       0.20      0.26      0.22        35\n",
      "          31       0.29      0.18      0.22        11\n",
      "          32       0.33      0.22      0.27         9\n",
      "          33       0.00      0.00      0.00        12\n",
      "          34       0.61      0.75      0.67        36\n",
      "          35       0.36      0.80      0.50         5\n",
      "          36       0.00      0.00      0.00         4\n",
      "          37       0.50      0.67      0.57         6\n",
      "          38       0.21      1.00      0.35         4\n",
      "          39       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.35      2382\n",
      "   macro avg       0.30      0.40      0.33      2382\n",
      "weighted avg       0.36      0.35      0.34      2382\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "# Calculate the confusion matrix\n",
    "# conf_matrix = metrics.confusion_matrix(y_test, pred)\n",
    "\n",
    "print(score)\n",
    "print(classification_report(y_test, pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
