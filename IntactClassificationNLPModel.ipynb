{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intact Medical Data ML model using Naive Bayes Classification Model\n",
    "\n",
    "### By: Daniyal, Hibah, Abhishek and Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our CxC Data Hackathon project, we were given medical transcription data by Intact and our goal was to predict which of the 40 provided medical specialties each transcription should be assigned to. This is our multiclass classification problem. We are judged based off the macro f1-score. Here are the steps we took to maximize the f1-score:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries and read in the Dataset to train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size with duplicates:  3969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/daniyalmohammed/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/daniyalmohammed/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/daniyalmohammed/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/daniyalmohammed/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "## Reading in our data\n",
    "df = pd.read_csv(\"./IntactInstructions/new_train.csv\", index_col=0)\n",
    "print(\"Test size with duplicates: \", len(df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Pre-process our data\n",
    "\n",
    "This is one of the most important steps. The ML model is only as good as its dataset, so we're going to make sure it's clean.\n",
    "\n",
    "All of the basic pre-processing is done by the CountVectorizer. These tasks include:\n",
    "- Tokenize (divide words individually)\n",
    "- Remove stop-words (remove \"the, and, to, or, ...\"; other special characters)\n",
    "- Lemmatize (convert similar words into its base root; eating, eats, ate => eat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label size:  3969\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       1\n",
       "2       1\n",
       "3       2\n",
       "4       0\n",
       "       ..\n",
       "3995    4\n",
       "3996    1\n",
       "3997    1\n",
       "3998    5\n",
       "3999    1\n",
       "Name: labels, Length: 3969, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create labels/target values\n",
    "y = df.labels\n",
    "print(\"Label size: \", len(y))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train size:  1587\n",
      "y_train size:  1587\n",
      "X_test size:  2382\n",
      "y_test size:  2382\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"transcription\"], y, test_size=0.6, random_state=42)\n",
    "\n",
    "# X_train: training data of features\n",
    "print(\"X_train size: \", len(X_train))\n",
    "# y_train: training data of label\n",
    "print(\"y_train size: \", len(y_train))\n",
    "\n",
    "# X_test: test data of features\n",
    "print(\"X_test size: \", len(X_test))\n",
    "# y_test: test data of label\n",
    "print(\"y_test size: \", len(y_test))\n",
    "\n",
    "# X_train\n",
    "# y_train[:50]\n",
    "# X_test\n",
    "# y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.feature_extraction.text.CountVectorizer'>\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# Custom pre-processing function\n",
    "def preprocess_data(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d|_', '', text) # removes digits and '_'\n",
    "    text = wordnet_lemmatizer.lemmatize(text)\n",
    "    return text\n",
    "\n",
    "# , preprocessor=preprocess_data\n",
    "# Initialize a CountVectorizer object\n",
    "count_vectorizer = CountVectorizer(stop_words=\"english\", preprocessor=preprocess_data, max_df=0.2, min_df=20, ngram_range=(1, 1))\n",
    "\n",
    "print(type(count_vectorizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Fit and Transform the Data\n",
    "\n",
    "Specifically, we must fit AND transform the feature training data and only transform the feature test data.\n",
    "This is a preliminary step.\n",
    "\n",
    "In fit_transform(), what happens is that we calculate the mean and variance of the training data and standardize the entire dataset (hence, transform). We only need transform() for the test data because we are using the mean and variance of the training data to standardize the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words:  2451\n",
      "['abc' 'abcd' 'abdominal' 'ability' 'able' 'abnormal' 'abnormalities'\n",
      " 'abnormality' 'abscess' 'absent' 'abuse' 'ac' 'access' 'accident'\n",
      " 'accommodate' 'accommodation' 'accompanied' 'accomplished' 'according'\n",
      " 'ace' 'achieved' 'acid' 'active' 'activities' 'activity' 'actually'\n",
      " 'acute' 'adaptic' 'add' 'added' 'addition' 'additional' 'additionally'\n",
      " 'address' 'adenocarcinoma' 'adenopathy' 'adequate' 'adequately'\n",
      " 'adhesions' 'adjacent' 'administered' 'administration' 'admission'\n",
      " 'admit' 'admits' 'admitted' 'admitting' 'adnexal' 'adrenal' 'adult'\n",
      " 'advance' 'advanced' 'advised' 'afebrile' 'affect' 'african' 'afternoon'\n",
      " 'age' 'aggressive' 'ago' 'agree' 'agreed' 'ahead' 'aid' 'air' 'airway'\n",
      " 'albumin' 'albuterol' 'alcohol' 'alert' 'alignment' 'alkaline' 'allergic'\n",
      " 'allergies' 'allergy' 'allis' 'allograft' 'allow' 'allowed' 'allowing'\n",
      " 'alt' 'alternative' 'alternatives' 'ambulate' 'ambulation' 'american'\n",
      " 'amounts' 'amoxicillin' 'analysis' 'anastomosis' 'anatomic' 'anatomy'\n",
      " 'ancef' 'anemia' 'anesthetic' 'anesthetized' 'aneurysm' 'angina' 'angio'\n",
      " 'angiogram' 'angiography' 'angioplasty' 'angle' 'anicteric' 'ankle'\n",
      " 'ankles' 'answered' 'anteriorly' 'anti' 'antibiotic' 'antibiotics'\n",
      " 'anticoagulation' 'anxiety' 'aorta' 'aortic' 'ap' 'apex' 'apical' 'apnea'\n",
      " 'apparent' 'apparently' 'appear' 'appearance' 'appeared' 'appearing'\n",
      " 'appears' 'appendectomy' 'appendix' 'appetite' 'application' 'applied'\n",
      " 'appointment' 'appreciated' 'approach' 'appropriate' 'appropriately'\n",
      " 'approximate' 'approximated' 'arch' 'areas' 'arm' 'arms' 'arterial'\n",
      " 'arteries' 'artery' 'arthritis' 'arthroplasty' 'articular' 'ascending'\n",
      " 'asked' 'aspect' 'aspects' 'aspirated' 'aspiration' 'aspirin' 'assess'\n",
      " 'assessed' 'assessment' 'assist' 'assistance' 'assistant' 'assisted'\n",
      " 'associated' 'ast' 'asthma' 'asymmetry' 'ataxia' 'atelectasis'\n",
      " 'atraumatic' 'atrial' 'atrium' 'atrophy' 'attached' 'attachment'\n",
      " 'attachments' 'attack' 'attempt' 'attempted' 'attempts' 'attention'\n",
      " 'atypical' 'auditory' 'august' 'auscultation' 'available' 'avoid' 'awake'\n",
      " 'awakened' 'aware' 'away' 'axial' 'axilla' 'axillary' 'axis' 'babinski'\n",
      " 'baby' 'bacitracin' 'background' 'bag' 'balance' 'ball' 'balloon' 'band'\n",
      " 'bandage' 'base' 'based' 'baseline' 'bases' 'basis' 'beats' 'bed'\n",
      " 'bedside' 'bedtime' 'began' 'beginning' 'begun' 'behavior' 'believe'\n",
      " 'believes' 'beneath' 'benefit' 'benefits' 'benign' 'best' 'beta'\n",
      " 'betadine' 'better' 'bicarbonate' 'biceps' 'bifurcation' 'bilateral'\n",
      " 'bimanual' 'biopsies' 'biopsy' 'bipolar' 'birth' 'bit' 'black' 'bladder'\n",
      " 'blade' 'bleed' 'bleeding' 'block' 'blunt' 'bluntly' 'bodies' 'body'\n",
      " 'bolus' 'bone' 'bones' 'bony' 'border' 'borderline' 'born' 'bovie'\n",
      " 'bowel' 'boy' 'bp' 'brachial' 'brain' 'branch' 'branches' 'breast'\n",
      " 'breasts' 'breath' 'breathing' 'brief' 'briefly' 'bring' 'broad'\n",
      " 'bronchoscopy' 'brother' 'bruising' 'bruit' 'bruits' 'bue' 'bulb'\n",
      " 'bulging' 'bulk' 'bun' 'bundle' 'bur' 'buried' 'burning' 'burr' 'bypass'\n",
      " 'calcification' 'calcified' 'calcium' 'calculated' 'calf' 'caliber'\n",
      " 'called' 'came' 'camera' 'canal' 'canals' 'cancer' 'candidate' 'cannula'\n",
      " 'cannulate' 'cannulated' 'capacity' 'capillary' 'capsular' 'capsule'\n",
      " 'carcinoma' 'cardiac' 'cardiology' 'cardiopulmonary' 'cardiovascular'\n",
      " 'careful' 'carefully' 'carotid' 'carpal' 'carried' 'cartilage' 'case'\n",
      " 'cataract' 'cath' 'catheter' 'catheterization' 'catheters' 'caucasian'\n",
      " 'cause' 'caused' 'causing' 'cauterized' 'cautery' 'cava' 'cavity' 'cbc'\n",
      " 'cc' 'cecum' 'cell' 'cells' 'center' 'central' 'cephalad' 'cephalic'\n",
      " 'cerebellar' 'cerebral' 'certainly' 'cervical' 'cervix' 'chair' 'chamber'\n",
      " 'change' 'changed' 'changes' 'chart' 'check' 'checked' 'chemotherapy'\n",
      " 'chief' 'child' 'children' 'chills' 'chloride' 'cholecystectomy'\n",
      " 'cholesterol' 'chromic' 'chronic' 'cigarettes' 'circumferential'\n",
      " 'circumflex' 'clamp' 'clamped' 'clamps' 'clean' 'cleaned' 'cleansed'\n",
      " 'cleared' 'clearly' 'clinic' 'clinical' 'clinically' 'clips' 'clock'\n",
      " 'close' 'closely' 'closing' 'closure' 'clots' 'clubbing' 'cn' 'cns'\n",
      " 'code' 'codeine' 'cognitive' 'cold' 'collateral' 'colli' 'colon'\n",
      " 'colonoscope' 'colonoscopy' 'color' 'combination' 'come' 'comes'\n",
      " 'comfortable' 'coming' 'common' 'compared' 'comparison' 'compartment'\n",
      " 'compatible' 'complain' 'complained' 'complaining' 'complains'\n",
      " 'complaint' 'complaints' 'complete' 'completed' 'completely' 'completion'\n",
      " 'complex' 'complicated' 'complication' 'component' 'compression'\n",
      " 'concern' 'concerned' 'concerning' 'concerns' 'conclusion' 'conditions'\n",
      " 'conduction' 'confirm' 'confirmed' 'confusion' 'congenital' 'congestion'\n",
      " 'congestive' 'conjunctiva' 'conjunctivae' 'connected' 'conscious'\n",
      " 'consciousness' 'consent' 'conservative' 'consider' 'considered'\n",
      " 'consisted' 'consistent' 'consisting' 'constant' 'constipation'\n",
      " 'constitutional' 'consult' 'consultation' 'consulted' 'contact' 'content'\n",
      " 'contents' 'continue' 'continued' 'continues' 'continuous' 'contour'\n",
      " 'contrast' 'control' 'controlled' 'cooperative' 'coord' 'coordination'\n",
      " 'copd' 'copious' 'copiously' 'cord' 'cords' 'coronal' 'coronary'\n",
      " 'correct' 'correction' 'cortex' 'cortical' 'cough' 'coumadin'\n",
      " 'counseling' 'count' 'counter' 'counts' 'couple' 'course' 'covered'\n",
      " 'crackles' 'cranial' 'craniotomy' 'cream' 'crease' 'create' 'created'\n",
      " 'creatinine' 'critical' 'cross' 'cruciate' 'crystalloid' 'csf']\n"
     ]
    }
   ],
   "source": [
    "# Fit and transform the TRAINING data using only the 'transciption' column values\n",
    "count_train = count_vectorizer.fit_transform(X_train.values)\n",
    "# Transform the TEST data using only the 'transciption' column values\n",
    "count_test = count_vectorizer.transform(X_test.values)\n",
    "\n",
    "\n",
    "# Print number of words processing\n",
    "print(\"Number of words: \", len(count_vectorizer.get_feature_names_out())) # number of test data from split\n",
    "# Print the features (individual tokens) of the count_vectorizer\n",
    "print(count_vectorizer.get_feature_names_out()[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Fine-tune Parameters and Choose the Best Classification Model\n",
    "\n",
    "Fine-tuning parameters and selecting the best classification model is important because it can greatly improve the performance and accuracy of a machine learning model.\n",
    "\n",
    "When building a classification model, there are typically many different algorithms and parameters that can be used to train the model. Different algorithms may be more suitable for different types of data, and adjusting the parameters of a particular algorithm can also have a significant impact on its performance.\n",
    "\n",
    "Fine-tuning the parameters of a model involves adjusting the settings that control how the model learns and makes predictions, such as the learning rate, regularization parameters, or the number of hidden layers in a neural network. By optimizing these parameters, we can ensure that the model is better able to learn the underlying patterns in the data, and that it can make more accurate predictions.\n",
    "\n",
    "Similarly, choosing the best classification model involves selecting the algorithm that is most suited to the particular problem we are trying to solve. For example, some algorithms may work better with binary classification problems, while others may be more appropriate for multi-class classification tasks. By choosing the best algorithm, we can improve the accuracy and performance of our model, and ensure that it is able to generalize well to new data.\n",
    "\n",
    "Overall, fine-tuning parameters and selecting the best classification model is an essential step in building effective machine learning models, and can help to ensure that they are able to make accurate predictions and deliver real-world value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting... Naive Bayes\n",
      "Best parameters: {'alpha': 0.1}\n",
      "Testing F1-macro score: 0.276\n",
      "--------------------------------------------------------------------------------\n",
      "starting... Decision Tree\n",
      "Best parameters: {'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "Testing F1-macro score: 0.081\n",
      "--------------------------------------------------------------------------------\n",
      "starting... K-Nearest Neighbors\n",
      "Best parameters: {'n_neighbors': 5}\n",
      "Testing F1-macro score: 0.140\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load the CSV file into a pandas dataframe\n",
    "df1 = pd.read_csv('./IntactInstructions/new_train.csv')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(df['transcription'], df['medical_specialty'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a CountVectorizer to convert the text into a bag-of-words representation\n",
    "vectorizer1 = CountVectorizer(max_features=10000)\n",
    "X_train_vectors1 = vectorizer1.fit_transform(X_train1)\n",
    "X_test_vectors1 = vectorizer1.transform(X_test1)\n",
    "\n",
    "# Define the hyperparameters to tune for each classifier\n",
    "nb_params = {'alpha': [0.1, 0.5, 1.0]}\n",
    "#svm_params = {'C': [0.1, 0.5, 1.0]}\n",
    "dt_params = {'max_depth': [5, 10, 15], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]}\n",
    "#gb_params = {'n_estimators': [50, 100, 200], 'max_depth': [5, 10, 15], 'learning_rate': [0.1, 0.5, 1.0]}\n",
    "#rf_params = {'n_estimators': [50, 100, 200], 'max_depth': [5, 10, 15], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]}\n",
    "knn_params = {'n_neighbors': [3, 5, 7]}\n",
    "#mlp_params = {'hidden_layer_sizes': [(100,), (100, 50), (100, 50, 25)], 'alpha': [0.0001, 0.001, 0.01]}\n",
    "#xgb_params = {'n_estimators': [50, 100, 200], 'max_depth': [5, 10, 15], 'learning_rate': [0.1, 0.5, 1.0]}\n",
    "\n",
    "# Train and evaluate each classifier using grid search and cross-validation\n",
    "classifiers = [('Naive Bayes', MultinomialNB(), nb_params),\n",
    "               #('Support Vector Machines', LinearSVC(), svm_params),\n",
    "               ('Decision Tree', DecisionTreeClassifier(), dt_params),\n",
    "               #('Gradient Boosting', GradientBoostingClassifier(), gb_params),\n",
    "               #('Random Forest', RandomForestClassifier(), rf_params),\n",
    "               ('K-Nearest Neighbors', KNeighborsClassifier(), knn_params),\n",
    "               #('Multi-Layer Perceptron', MLPClassifier(), mlp_params),\n",
    "               #('XGBoost', XGBClassifier(), xgb_params)\n",
    "               ]\n",
    "\n",
    "for name, clf, params in classifiers:\n",
    "    print(f'starting... {name}')\n",
    "    grid_search = GridSearchCV(clf, params, cv=5, n_jobs=-1, scoring='f1_macro')\n",
    "    grid_search.fit(X_train_vectors1, y_train1)\n",
    "    y_pred1 = grid_search.predict(X_test_vectors1)\n",
    "    f1_macro = f1_score(y_test1, y_pred1, average='macro')\n",
    "    print(f'Best parameters: {grid_search.best_params_}')\n",
    "    print(f'Testing F1-macro score: {f1_macro:.3f}')\n",
    "    print('-' * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have commented out some of the classifications for runtime purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Train our models here\n",
    "\n",
    "From our previous work, we can see that the Multinomial Naive Bayes is the most accurate model to classify our labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of predictions:  2382\n",
      "[ 7  6 34 ...  9  3 20]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a Multinomial Naive Bayes classifier\n",
    "nb_clf = MultinomialNB(alpha=0.4)\n",
    "# Fit the classifier to the training data\n",
    "nb_clf.fit(count_train, y_train)\n",
    "# Create the predicted tags\n",
    "pred = nb_clf.predict(count_test)\n",
    "\n",
    "# Print the predictions for each row of the dataset (1001 rows)\n",
    "print(\"Number of predictions: \", len(pred)) # Equal to the number of test data (when it got split)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Evaluate the model\n",
    "\n",
    "We will create an accuracy score and also a confusion matrix.\n",
    "\n",
    "Precision = TP/(TP + FP)\n",
    "\n",
    "Recall = TP/(TP+FN)\n",
    "\n",
    "F1 Score = 2*(Recall * Precision) / (Recall + Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35684298908480266\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.10      0.14        42\n",
      "           1       0.36      0.20      0.25       485\n",
      "           2       0.38      0.34      0.36       119\n",
      "           3       0.30      0.54      0.39        24\n",
      "           4       0.35      0.38      0.37        95\n",
      "           5       0.41      0.47      0.44       106\n",
      "           6       0.44      0.41      0.43       174\n",
      "           7       0.47      0.34      0.39       196\n",
      "           8       0.27      0.24      0.25        38\n",
      "           9       0.48      0.57      0.52        53\n",
      "          10       0.21      0.33      0.25       132\n",
      "          11       0.21      0.17      0.19        41\n",
      "          12       0.28      0.50      0.36        14\n",
      "          13       0.23      0.25      0.24        73\n",
      "          14       0.20      0.14      0.17         7\n",
      "          15       0.60      0.74      0.66        34\n",
      "          16       0.30      0.30      0.30       254\n",
      "          17       0.17      0.33      0.22         3\n",
      "          18       0.35      0.55      0.43        47\n",
      "          19       0.42      0.62      0.50        76\n",
      "          20       0.44      0.47      0.45        87\n",
      "          21       0.37      0.50      0.42        52\n",
      "          22       0.67      0.80      0.73         5\n",
      "          23       0.33      0.17      0.22        12\n",
      "          24       0.33      0.27      0.30        11\n",
      "          25       0.22      0.33      0.27        24\n",
      "          26       0.00      0.00      0.00         2\n",
      "          27       0.48      0.78      0.59        40\n",
      "          28       0.23      1.00      0.38         3\n",
      "          29       0.37      0.78      0.50         9\n",
      "          30       0.18      0.26      0.21        35\n",
      "          31       0.29      0.18      0.22        11\n",
      "          32       0.33      0.33      0.33         9\n",
      "          33       0.33      0.25      0.29        12\n",
      "          34       0.63      0.81      0.71        36\n",
      "          35       0.29      0.80      0.42         5\n",
      "          36       0.00      0.00      0.00         4\n",
      "          37       0.50      0.67      0.57         6\n",
      "          38       0.20      1.00      0.33         4\n",
      "          39       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.36      2382\n",
      "   macro avg       0.32      0.42      0.35      2382\n",
      "weighted avg       0.36      0.36      0.35      2382\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "# Calculate the confusion matrix\n",
    "# conf_matrix = metrics.confusion_matrix(y_test, pred)\n",
    "\n",
    "print(score)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Try the Test Data and Get the Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, our final results indicate that our macro average of classification guessing is 0.35. Given the turbulent data set and the many duplicate answers, these results are likely our largest possible macro f1-score. Further next steps would be to clean the data further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the test data\n",
    "test_data = pd.read_csv(\"./IntactInstructions/new_test.csv\")\n",
    "\n",
    "# Preprocess the test data\n",
    "test_counts = count_vectorizer.transform(test_data[\"transcription\"])\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_preds = nb_clf.predict(test_counts)\n",
    "\n",
    "# Format the predictions as desired\n",
    "output = \"\"\n",
    "for i in range(len(test_preds)):\n",
    "    output += str(i) + \",\" + str(test_preds[i]) + \"\\n\"\n",
    "\n",
    "# Write the output to a file\n",
    "with open(\"predictions.csv\", \"w\") as file:\n",
    "    file.write(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
