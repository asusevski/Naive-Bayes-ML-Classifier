{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intact Medical Data ML model using Naive Bayes Classification Model\n",
    "\n",
    "### By: Daniyal, Hibah, Abhishek and Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our Data Hackathon project, we were given medical transcription data by Intact to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import libraries and read in the data\n",
    "\n",
    "We'll add more libraries, as we move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size with duplicates:  3969\n",
      "Test size without duplicates:  2255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/daniyalmohammed/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/daniyalmohammed/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/daniyalmohammed/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/daniyalmohammed/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>medical_specialty</th>\n",
       "      <th>transcription</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Emergency Room Reports</td>\n",
       "      <td>REASON FOR THE VISIT:,  Very high PT/INR.,HIST...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Surgery</td>\n",
       "      <td>PREOPERATIVE DIAGNOSIS:,  Acetabular fracture ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Surgery</td>\n",
       "      <td>NAME OF PROCEDURE,1.  Selective coronary angio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Radiology</td>\n",
       "      <td>REFERRING DIAGNOSIS: , Motor neuron disease.,P...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Emergency Room Reports</td>\n",
       "      <td>CHIEF COMPLAINT: , Dental pain.,HISTORY OF PRE...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3957</th>\n",
       "      <td>Orthopedic</td>\n",
       "      <td>PREOPERATIVE DIAGNOSIS: , Right hip osteoarthr...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3959</th>\n",
       "      <td>Surgery</td>\n",
       "      <td>PREOPERATIVE DIAGNOSIS: , Left knee medial fem...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3975</th>\n",
       "      <td>Surgery</td>\n",
       "      <td>DELIVERY NOTE: , The patient is a very pleasan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3977</th>\n",
       "      <td>Urology</td>\n",
       "      <td>PREOPERATIVE DX: , Stress urinary incontinence...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>Surgery</td>\n",
       "      <td>PREOPERATIVE DIAGNOSIS: , Right pleural effusi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2255 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            medical_specialty  \\\n",
       "0      Emergency Room Reports   \n",
       "1                     Surgery   \n",
       "2                     Surgery   \n",
       "3                   Radiology   \n",
       "4      Emergency Room Reports   \n",
       "...                       ...   \n",
       "3957               Orthopedic   \n",
       "3959                  Surgery   \n",
       "3975                  Surgery   \n",
       "3977                  Urology   \n",
       "3999                  Surgery   \n",
       "\n",
       "                                          transcription  labels  \n",
       "0     REASON FOR THE VISIT:,  Very high PT/INR.,HIST...       0  \n",
       "1     PREOPERATIVE DIAGNOSIS:,  Acetabular fracture ...       1  \n",
       "2     NAME OF PROCEDURE,1.  Selective coronary angio...       1  \n",
       "3     REFERRING DIAGNOSIS: , Motor neuron disease.,P...       2  \n",
       "4     CHIEF COMPLAINT: , Dental pain.,HISTORY OF PRE...       0  \n",
       "...                                                 ...     ...  \n",
       "3957  PREOPERATIVE DIAGNOSIS: , Right hip osteoarthr...       6  \n",
       "3959  PREOPERATIVE DIAGNOSIS: , Left knee medial fem...       1  \n",
       "3975  DELIVERY NOTE: , The patient is a very pleasan...       1  \n",
       "3977  PREOPERATIVE DX: , Stress urinary incontinence...      20  \n",
       "3999  PREOPERATIVE DIAGNOSIS: , Right pleural effusi...       1  \n",
       "\n",
       "[2255 rows x 3 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "df = pd.read_csv(\"new_train.csv\", index_col=0)\n",
    "print(\"Test size with duplicates: \", len(df))\n",
    "# Get rid of duplicate transcriptions\n",
    "df = df.drop_duplicates(subset=['transcription'])\n",
    "print(\"Test size without duplicates: \", len(df))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Further process the dataframe\n",
    "\n",
    "# # Get rid of duplicate transcriptions\n",
    "# # df = df.drop_duplicates(subset=['transcription'])\n",
    "# print(\"Test size without duplicates: \", len(df))\n",
    "\n",
    "# start_patterns_soci = [\"SOCIAL HISTORY:,\", \"SOCIAL HISTORY: ,\", \"SOCIAL HISTORY:  ,\", \"SOCIAL HISTORY:   ,\", \"SOCIAL HISTORY,\", \"SOCIAL HISTORY ,\", \"SOCIAL HISTORY:\"]\n",
    "# start_patterns_fam = [\"FAMILY HISTORY:,\", \"FAMILY HISTORY: ,\", \"FAMILY HISTORY:  ,\", \"FAMILY HISTORY:   ,\", \"FAMILY HISTORY,\", \"FAMILY HISTORY ,\", \"FAMILY HISTORY:\"]\n",
    "# start_patterns = [start_patterns_soci, start_patterns_fam]\n",
    "\n",
    "# def remove_hist(transcription):\n",
    "#     for start_patterns_list in start_patterns:\n",
    "#         # Find the start position of the block of text\n",
    "#         start = -1\n",
    "#         dummy = 0\n",
    "#         for pattern in start_patterns_list:\n",
    "#             start = transcription.find(pattern)\n",
    "#             if start != -1:\n",
    "#                 dummy = len(pattern)\n",
    "#                 break\n",
    "    \n",
    "#         # Check if the start position is valid\n",
    "#         if start != -1:\n",
    "#             # Find the end position of the block of text\n",
    "#             end = transcription.find(\",\", start+dummy)\n",
    "#             if end == -1:\n",
    "#                 end = len(transcription)\n",
    "        \n",
    "\n",
    "#             # Extract the parts of the string that come before and after the block of text\n",
    "#             before = transcription[:start]\n",
    "#             after = transcription[end+1:]\n",
    "            \n",
    "#             # Join the remaining parts of the string\n",
    "#             new_string = before + after\n",
    "#             transcription = new_string\n",
    "#         else:\n",
    "#             continue\n",
    "#     return transcription\n",
    "\n",
    "# count = 0\n",
    "# for row in df.iloc():\n",
    "#     count += 1\n",
    "#     # print(row['transcription'], '\\n')\n",
    "#     transcription = remove_hist(row['transcription'])\n",
    "#     print(transcription, '\\n')\n",
    "# print(\"Test size: \", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Pre-process our data\n",
    "\n",
    "I think this is the most important step here, the ML model is only as good as its dataset, so we gotta make sure it's squeaky clean.\n",
    "\n",
    "All of the basic pre-processing is done by the CountVectorizer, these tasks include:\n",
    "- Tokenize (divide words individually)\n",
    "- Remove stop-words (remove \"the, and, to, or, ...\"; other special characters)\n",
    "- Lemmatize (convert similar words into its base root; eating, eats, ate => eat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label size:  2255\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        1\n",
       "2        1\n",
       "3        2\n",
       "4        0\n",
       "        ..\n",
       "3957     6\n",
       "3959     1\n",
       "3975     1\n",
       "3977    20\n",
       "3999     1\n",
       "Name: labels, Length: 2255, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create labels/target values\n",
    "y = df.labels\n",
    "print(\"Label size: \", len(y))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train size:  1804\n",
      "y_train size:  1804\n",
      "X_test size:  451\n",
      "y_test size:  451\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"transcription\"], y, test_size=0.2, random_state=42)\n",
    "\n",
    "# X_train: training data of features\n",
    "print(\"X_train size: \", len(X_train))\n",
    "# y_train: training data of label\n",
    "print(\"y_train size: \", len(y_train))\n",
    "\n",
    "# X_test: test data of features\n",
    "print(\"X_test size: \", len(X_test))\n",
    "# y_test: test data of label\n",
    "print(\"y_test size: \", len(y_test))\n",
    "\n",
    "# X_train\n",
    "# y_train[:50]\n",
    "# X_test\n",
    "# y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.feature_extraction.text.CountVectorizer'>\n",
      "strawberry\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "# we could try stemming as well\n",
    "\n",
    "with open(\"words_alpha.txt\") as word_file:\n",
    "    english_words = set(word.strip().lower() for word in word_file)\n",
    "with open(\"medical_terms.txt\") as word_file:\n",
    "    medical_words = set(word.strip().lower() for word in word_file)\n",
    "def is_english_word(word):\n",
    "    return ((word.lower() in english_words) or (word.lower() in medical_words))\n",
    "\n",
    "# Custom pre-processing function\n",
    "def preprocess_data(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d|_', '', text) # removes words with digits and '_'\n",
    "    text = wordnet_lemmatizer.lemmatize(text)\n",
    "    return text\n",
    "\n",
    "# , preprocessor=preprocess_data\n",
    "# Initialize a CountVectorizer object\n",
    "count_vectorizer = CountVectorizer(stop_words=\"english\", preprocessor=preprocess_data, max_df=0.3, min_df=21, ngram_range=(1, 2))\n",
    "\n",
    "print(type(count_vectorizer))\n",
    "\n",
    "print(wordnet_lemmatizer.lemmatize(\"strawberries\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Fit and Transform the Data\n",
    "\n",
    "Specifically, we must fit AND transform the feature training data and only transform the feature test data.\n",
    "This is a preliminary step.\n",
    "\n",
    "In fit_transform(), what happens is that we calculate the mean and variance of the training data and standardize the entire dataset (hence, transform). We only need transform() for the test data because we are using the mean and variance of the training data to standardize the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words:  3713\n",
      "['abc' 'abcd' 'abcd general' 'abdomen' 'abdomen pelvis' 'abdomen prepped'\n",
      " 'abdomen soft' 'abdominal' 'abdominal cavity' 'abdominal pain'\n",
      " 'abdominal wall' 'ability' 'able' 'abnormal' 'abnormalities'\n",
      " 'abnormality' 'abscess' 'absent' 'abuse' 'ac' 'access' 'accessory'\n",
      " 'accident' 'accommodation' 'accompanied' 'accomplished' 'according' 'ace'\n",
      " 'achieve' 'achieved' 'acid' 'active' 'activities' 'activity' 'actually'\n",
      " 'acute' 'acute distress' 'adaptic' 'add' 'added' 'addition' 'additional'\n",
      " 'additionally' 'addressed' 'adenocarcinoma' 'adenoidectomy' 'adenopathy'\n",
      " 'adequate' 'adequate general' 'adequately' 'adherent' 'adhesions'\n",
      " 'adjacent' 'administered' 'administered patient' 'administration'\n",
      " 'admission' 'admit' 'admits' 'admitted' 'admitted hospital' 'admitting'\n",
      " 'adnexa' 'adnexal' 'adrenal' 'adult' 'advair' 'advanced' 'advised'\n",
      " 'afebrile' 'affect' 'aforementioned' 'african' 'african american'\n",
      " 'afternoon' 'age' 'aggressive' 'ago' 'agree' 'agreed' 'ahead' 'aid' 'air'\n",
      " 'airway' 'albumin' 'albuterol' 'alcohol' 'alcohol use' 'alert'\n",
      " 'alert oriented' 'alignment' 'alkaline' 'allergic' 'allergies'\n",
      " 'allergies known' 'allergies medications' 'allergies review'\n",
      " 'allergies social' 'allergy' 'allis' 'allograft' 'allow' 'allowed'\n",
      " 'allowing' 'alt' 'alternative' 'alternatives' 'ambulate' 'american'\n",
      " 'american female' 'american male' 'amounts' 'amounts sterile'\n",
      " 'amoxicillin' 'analysis' 'anastomosis' 'anatomic' 'anatomy' 'ancef'\n",
      " 'anemia' 'anesthesia administered' 'anesthesia care' 'anesthesia general'\n",
      " 'anesthesia induced' 'anesthesia local' 'anesthesia obtained'\n",
      " 'anesthesia patient' 'anesthesia spinal' 'anesthetic'\n",
      " 'anesthetic administered' 'anesthetized' 'aneurysm' 'angina' 'angio'\n",
      " 'angiogram' 'angiography' 'angioplasty' 'angle' 'anicteric' 'ankle'\n",
      " 'ankles' 'answered' 'anterior' 'anterior abdominal' 'anterior aspect'\n",
      " 'anterior cervical' 'anterior chamber' 'anterior descending'\n",
      " 'anterior posterior' 'anteriorly' 'anti' 'antibiotic'\n",
      " 'antibiotic solution' 'antibiotics' 'antibiotics given' 'anticoagulation'\n",
      " 'antrum' 'anxiety' 'aorta' 'aortic' 'aortic valve' 'ap' 'ap lateral'\n",
      " 'apex' 'apgars' 'aphasia' 'apical' 'apnea' 'apparent' 'apparent distress'\n",
      " 'apparently' 'appear' 'appearance' 'appeared' 'appeared normal'\n",
      " 'appearing' 'appears' 'appendectomy' 'appendix' 'appetite' 'application'\n",
      " 'applied' 'applied patient' 'appointment' 'appreciated' 'approach'\n",
      " 'appropriate' 'appropriately' 'approximate' 'approximated'\n",
      " 'approximately' 'approximately cc' 'approximately cm'\n",
      " 'approximately days' 'approximately hours' 'approximately minutes'\n",
      " 'approximately ml' 'approximately mm' 'approximately months'\n",
      " 'approximately weeks' 'approximately years' 'april' 'arch' 'area'\n",
      " 'area patient' 'area prepped' 'areas' 'arm' 'arms' 'arrival' 'arterial'\n",
      " 'arteries' 'artery' 'artery bypass' 'artery disease' 'artery left'\n",
      " 'artery right' 'arthritis' 'arthroplasty' 'articular' 'ascending'\n",
      " 'ascending aorta' 'ask' 'asked' 'aspect' 'aspect left' 'aspect right'\n",
      " 'aspects' 'aspirated' 'aspiration' 'aspirin' 'aspirin mg' 'assess'\n",
      " 'assessed' 'assessment' 'assessment patient' 'assessment plan' 'assist'\n",
      " 'assistance' 'assisted' 'associated' 'ast' 'ast alt' 'asthma' 'asymmetry'\n",
      " 'ataxia' 'atelectasis' 'ativan' 'atraumatic' 'atraumatic normocephalic'\n",
      " 'atrial' 'atrial fibrillation' 'atrium' 'atrophy' 'attached' 'attachment'\n",
      " 'attachments' 'attack' 'attempt' 'attempted' 'attempts' 'attending'\n",
      " 'attention' 'attention directed' 'attention turned' 'auditory' 'august'\n",
      " 'auscultation' 'auscultation bilaterally' 'auscultation heart'\n",
      " 'auscultation percussion' 'av' 'available' 'average' 'avoid' 'awake'\n",
      " 'awake alert' 'awakened' 'aware' 'away' 'axial' 'axilla' 'axillary'\n",
      " 'axis' 'babinski' 'baby' 'bacitracin' 'bactrim' 'bag' 'balance' 'balloon'\n",
      " 'band' 'bandage' 'basal' 'base' 'based' 'baseline' 'basis' 'beats'\n",
      " 'beats minute' 'bed' 'bedside' 'bedtime' 'began' 'begin' 'beginning'\n",
      " 'begun' 'behavior' 'believe' 'beneath' 'benefit' 'benefits'\n",
      " 'benefits alternatives' 'benefits procedure' 'benign' 'best' 'beta'\n",
      " 'betadine' 'betadine solution' 'better' 'bicarbonate' 'biceps'\n",
      " 'bifurcation' 'bilateral' 'bilateral lower' 'bilaterally' 'bile'\n",
      " 'bilirubin' 'bimanual' 'biopsies' 'biopsies taken' 'biopsy' 'bipolar'\n",
      " 'birth' 'bit' 'black' 'bladder' 'bladder flap' 'blade' 'blade scalpel'\n",
      " 'blade used' 'bleed' 'bleeding' 'bleeding infection' 'block' 'blocker'\n",
      " 'blood cell' 'blood cells' 'blood loss' 'blood pressure' 'blood sugar'\n",
      " 'blue' 'blunt' 'blunt dissection' 'bluntly' 'bodies' 'body' 'bolus'\n",
      " 'bone' 'bones' 'bony' 'bony prominences' 'border' 'borderline' 'born'\n",
      " 'bovie' 'bovie cautery' 'bovie electrocautery' 'bowel' 'bowel bladder'\n",
      " 'bowel movements' 'bowel sounds' 'boy' 'bp' 'bp hr' 'brain' 'branch'\n",
      " 'branches' 'breast' 'breast cancer' 'breasts' 'breath' 'breath chest'\n",
      " 'breath sounds' 'breathing' 'brief' 'brief history' 'briefly' 'bright'\n",
      " 'bring' 'broad' 'bronchitis' 'bronchoscopy' 'brother' 'brothers'\n",
      " 'brought' 'brought operating' 'brought operative' 'brought recovery'\n",
      " 'bruising' 'bruit' 'bruits' 'bulb' 'bulging' 'bulk' 'bun'\n",
      " 'bun creatinine' 'bur' 'buried' 'burning' 'burr' 'bypass' 'calcification'\n",
      " 'calcified' 'calcium' 'calculated' 'calf' 'caliber' 'called' 'came'\n",
      " 'camera' 'canal' 'canals' 'cancer' 'candidate' 'cannula' 'cannulate'\n",
      " 'cannulated' 'capacity' 'capillary' 'capillary refill' 'capsular'\n",
      " 'capsule' 'carcinoma' 'cardiac' 'cardiac catheterization' 'cardiology'\n",
      " 'cardiovascular' 'cardiovascular heart' 'cardiovascular regular' 'care'\n",
      " 'care patient' 'care physician' 'care taken' 'care unit' 'careful'\n",
      " 'carefully' 'carefully dissected' 'carotid' 'carotid artery'\n",
      " 'carotid bruits' 'carpal' 'carpal tunnel' 'carried' 'cartilage' 'case'\n",
      " 'cat' 'cataract' 'cath' 'catheter' 'catheter advanced'\n",
      " 'catheter inserted' 'catheter placed' 'catheter removed'\n",
      " 'catheterization' 'catheters' 'caucasian' 'caucasian female'\n",
      " 'caucasian male' 'caudal' 'cause' 'caused' 'causes' 'causing'\n",
      " 'cauterized' 'cautery' 'cautery used' 'cava' 'cavity' 'cbc' 'cc'\n",
      " 'cc marcaine' 'cecum' 'cell' 'cell carcinoma' 'cell count' 'cells'\n",
      " 'cellulitis' 'center' 'central' 'cephalad' 'cephalic' 'cerebellar'\n",
      " 'cerebral' 'certainly' 'cervical' 'cervical discectomy' 'cervical spine'\n",
      " 'cervix' 'cervix grasped']\n"
     ]
    }
   ],
   "source": [
    "# Fit and transform the TRAINING data using only the 'transciption' column values\n",
    "count_train = count_vectorizer.fit_transform(X_train.values)\n",
    "# Transform the TEST data using only the 'transciption' column values\n",
    "count_test = count_vectorizer.transform(X_test.values)\n",
    "\n",
    "\n",
    "# Print number of words processing\n",
    "print(\"Number of words: \", len(count_vectorizer.get_feature_names_out())) # number of test data from split\n",
    "# Print the features (individual tokens) of the count_vectorizer\n",
    "print(count_vectorizer.get_feature_names_out()[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Train our models here\n",
    "\n",
    "We used the Multinomial Naive Bayes to classify our labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of predictions:  451\n",
      "[20 16  1  1 28  7 15 16 20  5 25 10 27 22  1  6 16  0 19 11 16  1 13  7\n",
      " 16 13 10 34  1 20  7 16 29  5 13 10 11 16 13 15 21  2 15  6  1  9 10 15\n",
      " 13 34 20  2  1  6  6  1  0  6  4  0 23 10  7 19  6  2  1 16 25 27  6 19\n",
      "  7  1  1  7  2  4 27  6 16  2  7  0  6 18 18 21 16 10 20  9 19 27 10 16\n",
      " 13  4  4 29 16 27 16  5  1  1  4 16  5  2 27  1 15 27 30  0  9 31 16 20\n",
      "  6  7  1 25 20 19 16 19 10 18  7  6  7  0 27 10  1 16  6  1  7 15 21 16\n",
      "  1  7 16 16  1  1  1  9  5  7  2  3  9 16 30 19  6  1  6 16 15 22  6  7\n",
      " 29 29  2 18 27  5 10  5  5 16  6 16  2 21 13 25  6 34  7 15 19 19  1  1\n",
      " 13 19 21  1 37  3 10  6 19  1 35 21  6  1 39  2 16  8  2  2  1  1 27 10\n",
      " 16 16  2 34 19  9  6  1  0 21 16  7  6 11 16 16  1  6  5 21 10  1 21 10\n",
      " 13 32  3 22  1  1  6 34  1  4 27 34  7 14 37 11  1 19  5  5  2  2 34 18\n",
      " 13  2  2 16  2  6 16  1 16  9 11  8  1  1 26  2  2 13  9 13  1 13 10 10\n",
      "  7 16  9  3 16 27  5 16 37 16  0  9  0 16 28 13  3  2  4  1 10  1 13 21\n",
      "  4 34 21 16  7  2 19  7  5  6  2 34  1 25 11  2  7  1 14  5  6 16  5  6\n",
      "  6 16 21 13 20  3 10  2  5 18 10 14 13  7 20 16  2 10 20 16  5 15  7 19\n",
      " 21  5  1  6 16 25  9 15 19 10 21 19  2 20  6 27  6 10 14  3  7 15 34  1\n",
      "  7 16 16  2  4 10  1 19  4  4  5  2  7 13 27  6 27  7 20 13 16 10  9 19\n",
      " 16 10 16  1  1 10  4 16  1  6 16  9 20 11 20 10  1 13  6 16  1  1  7  3\n",
      " 15  1  2 16 15  6  6 16 16  5 29 20 27  7 21  7 21  5 15]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a Multinomial Naive Bayes classifier\n",
    "nb_clf = MultinomialNB(alpha=0.4)\n",
    "# Fit the classifier to the training data\n",
    "nb_clf.fit(count_train, y_train)\n",
    "# Create the predicted tags\n",
    "pred = nb_clf.predict(count_test)\n",
    "\n",
    "# Print the predictions for each row of the dataset (1001 rows)\n",
    "print(\"Number of predictions: \", len(pred)) # Equal to the number of test data (when it got split)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Evaluate the model\n",
    "\n",
    "We will create an accuracy score and also a confusion matrix.\n",
    "\n",
    "Precision = TP/(TP + FP)\n",
    "\n",
    "Recall = TP/(TP+FN)\n",
    "\n",
    "F1 Score = 2*(Recall * Precision) / (Recall + Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.50      0.40         6\n",
      "           1       0.46      0.27      0.34        93\n",
      "           2       0.40      0.44      0.42        27\n",
      "           3       0.25      0.40      0.31         5\n",
      "           4       0.50      0.35      0.41        17\n",
      "           5       0.43      0.45      0.44        20\n",
      "           6       0.51      0.49      0.50        39\n",
      "           7       0.48      0.35      0.41        40\n",
      "           8       0.00      0.00      0.00         5\n",
      "           9       0.69      0.82      0.75        11\n",
      "          10       0.00      0.00      0.00         7\n",
      "          11       0.29      0.25      0.27         8\n",
      "          12       0.00      0.00      0.00         5\n",
      "          13       0.21      0.31      0.25        13\n",
      "          14       0.00      0.00      0.00         0\n",
      "          15       0.50      0.88      0.64         8\n",
      "          16       0.45      0.40      0.42        62\n",
      "          18       0.50      0.75      0.60         4\n",
      "          19       0.42      0.67      0.52        12\n",
      "          20       0.47      0.58      0.52        12\n",
      "          21       0.41      0.78      0.54         9\n",
      "          22       1.00      1.00      1.00         3\n",
      "          23       1.00      0.33      0.50         3\n",
      "          24       0.00      0.00      0.00         3\n",
      "          25       0.17      0.25      0.20         4\n",
      "          26       0.00      0.00      0.00         1\n",
      "          27       0.50      0.89      0.64         9\n",
      "          28       0.00      0.00      0.00         0\n",
      "          29       0.00      0.00      0.00         0\n",
      "          30       0.50      0.17      0.25         6\n",
      "          31       0.00      0.00      0.00         5\n",
      "          32       0.00      0.00      0.00         0\n",
      "          33       0.00      0.00      0.00         1\n",
      "          34       0.90      0.90      0.90        10\n",
      "          35       0.00      0.00      0.00         2\n",
      "          37       0.33      1.00      0.50         1\n",
      "          39       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.41       451\n",
      "   macro avg       0.32      0.36      0.32       451\n",
      "weighted avg       0.44      0.41      0.41       451\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a Multinomial Naive Bayes classifier\n",
    "nb_clf = MultinomialNB(alpha=0.45) # best: 0.4\n",
    "# Fit the classifier to the training data\n",
    "nb_clf.fit(count_train, y_train)\n",
    "# Create the predicted tags\n",
    "pred = nb_clf.predict(count_test)\n",
    "\n",
    "# Print the predictions for each row of the dataset (1001 rows)\n",
    "#print(\"Number of predictions: \", len(pred)) # Equal to the number of test data (when it got split)\n",
    "#print(pred)\n",
    "\n",
    "\n",
    "# Calculate the accuracy score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "# Calculate the confusion matrix\n",
    "# conf_matrix = metrics.confusion_matrix(y_test, pred)\n",
    "\n",
    "\n",
    "#print(score)\n",
    "print(classification_report(y_test, pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
